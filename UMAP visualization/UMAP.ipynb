{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "939f7f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\toxpred\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna import Trial\n",
    "\n",
    "from math import sqrt\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mordred import Calculator, descriptors\n",
    "#import openbabel\n",
    "from openbabel import pybel\n",
    "from PyBioMed.PyMolecule.fingerprint import CalculatePubChemFingerprint,CalculateECFP2Fingerprint\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdchem import Atom\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef, roc_curve, auc \n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as G_Loader \n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "\n",
    "# RDkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdmolops import GetAdjacencyMatrix\n",
    "\n",
    "from rdkit.Chem import MACCSkeys\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit import DataStructs\n",
    "\n",
    "# Pytorch and Pytorch Geometric\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F # activation function\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader as V_Loader # dataset management\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef, roc_curve, auc \n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# performances visualization \n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import statistics\n",
    "from prettytable import PrettyTable\n",
    "%run ./my_performances.ipynb \n",
    "\n",
    "\n",
    "#%run ./graph_feature.ipynb \n",
    "#%run ./dataset_processing.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1d86370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the data fingerprint \n",
    "#=======================================================\n",
    "k=10\n",
    "final_clean_fingerp_train=[]\n",
    "final_clean_fingerp_val=[]\n",
    "for i in range(k):\n",
    "    final_clean_fingerp_train.append(np.load('final_clean_fingerp_train'+ str(i)+'.npy'))\n",
    "    final_clean_fingerp_val.append(np.load('final_clean_fingerp_val' +str(i)+'.npy'))\n",
    "\n",
    "final_clean_fingerp_test = np.load('final_clean_fingerp_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8c1fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = np.load('train_indices.npy')\n",
    "val_idx = np.load('val_indices.npy')\n",
    "test_idx = np.load('test_indices.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8a2b26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the output label \n",
    "total_train_targets =[]\n",
    "total_validation_targets =[]\n",
    "total_test_targets=[]\n",
    "for i in range(k):\n",
    "    total_train_targets.append(np.load('total_train_targets'+ str(i)+'.npy'))\n",
    "    total_validation_targets.append(np.load('total_validation_targets' +str(i)+'.npy'))\n",
    "\n",
    "total_test_targets= np.load('total_test_targets.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "820cce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader for training (vector data)\n",
    "#======================================================================================\n",
    "list_data_fingerp_train =[]\n",
    "list_data_fingerp_val =[]\n",
    "list_data_target_train =[]\n",
    "list_data_target_val =[]\n",
    "\n",
    "for data_train, data_val, tr_targets, val_targets in zip(final_clean_fingerp_train, final_clean_fingerp_val,total_train_targets, total_validation_targets):\n",
    "    train_loader = V_Loader(dataset = data_train, batch_size = 126)\n",
    "    val_loader = V_Loader(dataset = data_val, batch_size = 126)\n",
    "    \n",
    "    tr_target_loader = V_Loader(dataset = tr_targets, batch_size = 126)\n",
    "    val_target_loader =  V_Loader(dataset = val_targets, batch_size = 126)\n",
    "    \n",
    "    list_data_fingerp_train.append(train_loader)\n",
    "    list_data_fingerp_val.append(val_loader)\n",
    "    \n",
    "    list_data_target_train.append(tr_target_loader)\n",
    "    list_data_target_val.append(val_target_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a0d557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load structured graph data\n",
    "train_idx = np.load('train_indices.npy')\n",
    "val_idx = np.load('val_indices.npy')\n",
    "test_idx = np.load('test_indices.npy')\n",
    "\n",
    "k=10\n",
    "data_list_train=[]\n",
    "for i in range(k):\n",
    "    one_fold= []\n",
    "    for idx in range(train_idx[i]):\n",
    "        one_graph_data =torch.load(f\"data_train_{i}/tensor{idx}.pt\")\n",
    "        one_fold.append(one_graph_data)\n",
    "    data_list_train.append(one_fold)\n",
    "    \n",
    "data_list_val=[]\n",
    "for i in range(k):\n",
    "    one_fold= []\n",
    "    for idx in range(val_idx[i]):\n",
    "        one_graph_data =torch.load(f\"data_val_{i}/tensor{idx}.pt\")\n",
    "        one_fold.append(one_graph_data)\n",
    "    data_list_val.append(one_fold)\n",
    "    \n",
    "data_list_test =[]\n",
    "for idx in range(test_idx):\n",
    "    one_graph_data =torch.load(f\"data_test/tensor{idx}.pt\")\n",
    "    data_list_test.append(one_graph_data)\n",
    "    \n",
    "# load the output label \n",
    "total_train_targets =[]\n",
    "total_validation_targets =[]\n",
    "total_test_targets=[]\n",
    "for i in range(k):\n",
    "    total_train_targets.append(np.load('total_train_targets'+ str(i)+'.npy'))\n",
    "    total_validation_targets.append(np.load('total_validation_targets' +str(i)+'.npy'))\n",
    "\n",
    "total_test_targets= np.load('total_test_targets.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7200eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader for training (graph data)\n",
    "#======================================================================================\n",
    "list_data_graph_train =[]\n",
    "list_data_graph_val =[]\n",
    "for data_train, data_val in zip(data_list_train, data_list_val):\n",
    "    train_loader = G_Loader(dataset = data_train, batch_size = 126)\n",
    "    val_loader = G_Loader(dataset = data_val, batch_size = 126)\n",
    "    list_data_graph_train.append(train_loader)\n",
    "    list_data_graph_val.append(val_loader)\n",
    "\n",
    "# create dataloader for test (graph data)\n",
    "#======================================================================================\n",
    "g_test_loader = G_Loader(dataset = data_list_test, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5de5c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "#define the loss function \n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27e8c2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input for forward the model x, edge_attr, edge_index, batch_index\n",
    "def train_1(g_loader,f_loader, combined_model, optimizer):\n",
    "    combined_model.train()\n",
    "    \n",
    "    for data_X1, data_X2 in zip(g_loader, f_loader):  # Iterate in batches over the training dataset.\n",
    "        \n",
    "        out = combined_model(data_X1.x, data_X1.edge_index, data_X1.batch, \n",
    "                             torch.tensor(data_X2, dtype=torch.float32))  # Perform a single forward pass.\n",
    "        \n",
    "        y_t = data_X1.y.type(torch.FloatTensor)\n",
    "        loss = criterion(out[:,0], y_t)  # Compute the loss.\n",
    "        #print(k)\n",
    "        #print(loss)\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "    return loss, combined_model, optimizer\n",
    "\n",
    "def validation_1(g_loader,f_loader,combined_model):\n",
    "    for data_X1, data_X2 in zip(g_loader,f_loader): # Iterate in batches over the training dataset.\n",
    "        out = combined_model(data_X1.x, data_X1.edge_index, data_X1.batch, \n",
    "                             torch.tensor(data_X2, dtype=torch.float32))  # Perform a single forward pass.\n",
    "        \n",
    "        y_t = data_X1.y.type(torch.FloatTensor)\n",
    "        val_loss = criterion(out[:,0], y_t)  # Compute the loss.\n",
    "        \n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da287a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_1(g_loader,f_loader,combined_model):\n",
    "    combined_model.eval()\n",
    "    list_pred =[]\n",
    "    list_targets =[]\n",
    "    list_A=[]\n",
    "    list_C =[]\n",
    "    correct = 0\n",
    "    for data_X1, data_X2 in zip(g_loader,f_loader): # Iterate in batches over the training dataset.\n",
    "        out,A,C = combined_model(data_X1.x, data_X1.edge_index, data_X1.batch, \n",
    "                             torch.tensor(data_X2, dtype=torch.float32))  # Perform a single forward pass.\n",
    "        out_1 = out[:,0] \n",
    "        list_pred.append(out_1.item())\n",
    "        list_targets.append(data_X1.y.item())\n",
    "        list_A.append(A.detach().numpy())\n",
    "        list_C.append(C.detach().numpy())\n",
    "        \n",
    "    return list_pred, list_targets, list_A, list_C \n",
    "\n",
    "# used to count the train accuracy ,and validation accuracy when in the training mode \n",
    "def test(g_loader,f_loader,combined_model):\n",
    "    combined_model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data_X1, data_X2 in zip(g_loader,f_loader): # Iterate in batches over the training dataset.\n",
    "        out = combined_model(data_X1.x, data_X1.edge_index, data_X1.batch, \n",
    "                             torch.tensor(data_X2, dtype=torch.float32))  # Perform a single forward pass.\n",
    "        out_1 = out[:,0]\n",
    "        for i,value in enumerate(out_1) :\n",
    "            if value > 0.5 :\n",
    "                out_1[i] = 1\n",
    "            else : out_1[i] = 0\n",
    "        pred = out_1  # Use the class with highest probability.\n",
    "        correct += int((pred == data_X1.y).sum())  # Check against ground-truth labels.\n",
    "    return correct / len(g_loader.dataset)  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f137b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(gnn_model, learning_rate, optimizer_type, weight_decay=1e-4):\n",
    "    if optimizer_type==1:\n",
    "        optimizer = torch.optim.SGD(gnn_model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    if optimizer_type==2:\n",
    "        optimizer = torch.optim.Adam(gnn_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    if optimizer_type ==3 :\n",
    "        optimizer = torch.optim.Adamax(gnn_model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4)\n",
    "        \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0449008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelA(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels1,hidden_channels2, num_node_features,heads1,heads2\n",
    "                 ,dropout_rateA,dropout_rateB, dropout_rateC,dense_layer1):\n",
    "        super(modelA, self).__init__()\n",
    "        \n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(num_node_features, hidden_channels1,heads1)\n",
    "        self.conv2 = GATConv(hidden_channels1*heads1,hidden_channels2, heads2)\n",
    "        \n",
    "        self.bn1 = BatchNorm (hidden_channels1*heads1)\n",
    "        self.bn2 = BatchNorm (hidden_channels2*heads2)\n",
    "        \n",
    "        self.dropoutA = dropout_rateA\n",
    "        self.dropoutB = dropout_rateB\n",
    "        self.dropoutC = dropout_rateC\n",
    "        \n",
    "        self.lin1 = Linear(hidden_channels2*heads2,dense_layer1)\n",
    "        #self.lin1 = Linear(hidden_channels2*heads2,dense_layer1)\n",
    "        self.lin2=  Linear(dense_layer1,1)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        \n",
    "        x = F.dropout(x, p=self.dropoutA , training=self.training)\n",
    "        x = x.relu()\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        x = F.dropout(x, p=self.dropoutB , training=self.training)\n",
    "        x = x.relu()\n",
    "        x = self.bn2(x)  \n",
    "        \n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        \n",
    "        A = x\n",
    "        \n",
    "        x = self.lin1(x)\n",
    "        x = F.dropout(x, p=self.dropoutC , training=self.training)\n",
    "        x = x.relu()\n",
    "        x = self.lin2(x)\n",
    "        return torch.sigmoid(x), A\n",
    "\n",
    "\n",
    "class modelB(torch.nn.Module):\n",
    "    def __init__(self, input_features, output_features,dropout_rateB1,dropout_rateB2,dropout_rateB3,  \n",
    "                 dense_layer1,dense_layer2, dense_layer3):\n",
    "        super(modelB, self).__init__()\n",
    "        self.lin1 = nn.Linear(input_features,dense_layer1)\n",
    "      \n",
    "        self.lin2 = nn.Linear(int(dense_layer1), dense_layer2)\n",
    "        self.lin3 = nn.Linear(int(dense_layer2), dense_layer3)\n",
    "        self.lin4 = nn.Linear(int(dense_layer3), output_features)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(int(dense_layer1))\n",
    "        self.bn2 = nn.BatchNorm1d(int(dense_layer2))\n",
    "        self.bn3 = nn.BatchNorm1d(int(dense_layer3))\n",
    "        self.dropoutB1 = dropout_rateB1\n",
    "        self.dropoutB2 = dropout_rateB2\n",
    "        self.dropoutB3 = dropout_rateB3\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = self.bn1(x)\n",
    "       \n",
    "        x = F.dropout(x, p= self.dropoutB1, training=self.training)\n",
    "        x = x.relu()\n",
    "  #      \n",
    "        x = self.lin2(x)\n",
    "        x = self.bn2(x)   \n",
    "        x = F.dropout(x, p= self.dropoutB2, training=self.training)\n",
    "        x = x.relu()\n",
    "  #      \n",
    "        x = self.lin3(x)\n",
    "        x = self.bn3(x)   \n",
    "        x = F.dropout(x, p= self.dropoutB3, training=self.training)\n",
    "\n",
    "        x = x.relu()\n",
    "        x = self.lin4(x)\n",
    "        return torch.sigmoid(x)        \n",
    "    \n",
    "class Combined_model(nn.Module):\n",
    "    def __init__(self, modelA, modelB, total_input_features, num_classes,dropout_rate_C, dense_layer_C):\n",
    "        super(Combined_model, self).__init__()\n",
    "        self.model_1 = modelA\n",
    "        self.model_2 = modelB\n",
    "        \n",
    "        \n",
    "        self.lin1 = Linear(total_input_features,int(dense_layer_C))\n",
    "        self.lin2 = Linear(int(dense_layer_C),num_classes)\n",
    "        \n",
    "        self.dropout_C = dropout_rate_C\n",
    "        \n",
    "    def forward(self, x1,edge_index, batch, x2):\n",
    "        xa,A = self.model_1(x1, edge_index, batch) # x1 node features in graph\n",
    "        xb= self.model_2(x2)                     # x2 is a vector features \n",
    "  #      xc= self.model_3(x3)                     # x2 is a vector features \n",
    "        \n",
    "        x = torch.cat((xa, xb), dim=1)\n",
    "        C = x\n",
    "  #      x = torch.cat((x, xc), dim=1)\n",
    "         # 3. Apply a final classifier\n",
    "        x = self.lin1(x)\n",
    "        x = F.dropout(x, p= self.dropout_C, training=self.training)\n",
    "        x = x.relu()\n",
    "        x = self.lin2(x)\n",
    "        return torch.sigmoid(x), A, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d640c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_param_d = {'dropout_rateB1': 0.19029920767006717,\n",
    "               'dropout_rateB2': 0.12142154817498285,\n",
    "               'dropout_rateB3': 0.30892320125476364,\n",
    "               'dense_layer1'  : 180,\n",
    "               'dense_layer2'  : 96,\n",
    "               'dense_layer3'  : 40,\n",
    "               'learning_rate' : 0.00014475405687104653,\n",
    "               'weight_decay'  : 0.0001839631656485908}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2523afcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_param_g = {'hidden_channels1': 82,\n",
    "                 'hidden_channels2': 82,\n",
    "                 'num_node_features': 79,\n",
    "                 'heads1':9,\n",
    "                 'heads2':9,\n",
    "                 'dropout_rateA':0.20184714781996305,\n",
    "                 'dropout_rateB':0.26157496401430025,\n",
    "                 'dropout_rateC':0.12826012269932247,\n",
    "                 'dense_layer1':64}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f52348ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_param_c = {'total_input_features':2,\n",
    "                 'num_classes':1,\n",
    "                 'dropout_rate_C':0.3054920980974637,\n",
    "                 'dense_layer_C':16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15d80976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the combined model \n",
    "k =10\n",
    "list_modelA = []\n",
    "list_modelB = []\n",
    "list_modelC = []\n",
    "\n",
    "# load model B\n",
    "for i in range(k):\n",
    "    \n",
    "    input_features    = 166 # length of feature data vector \n",
    "    output_features   = 1\n",
    "    \n",
    "    dropout_rateB1 =hyper_param_d['dropout_rateB1']\n",
    "    dropout_rateB2 =hyper_param_d['dropout_rateB2']\n",
    "    dropout_rateB3 =hyper_param_d['dropout_rateB3']\n",
    "    dense_layer1 = hyper_param_d['dense_layer1']\n",
    "    dense_layer2 = hyper_param_d['dense_layer2']\n",
    "    dense_layer3 = hyper_param_d['dense_layer3']\n",
    " \n",
    "        \n",
    "    model_b= modelB(input_features, output_features,dropout_rateB1,dropout_rateB2,dropout_rateB3,  \n",
    "                 dense_layer1,dense_layer2, dense_layer3)\n",
    "#    PATH = '0.39289model_fingerp'+ str(i)+'.pth'\n",
    "#    model_b.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "    list_modelB.append(model_b)\n",
    "\n",
    "# load model A\n",
    "for i in range(k):\n",
    "    hidden_channels1=hyper_param_g['hidden_channels1']\n",
    "    hidden_channels2=hyper_param_g['hidden_channels2']\n",
    "    num_node_features=hyper_param_g['num_node_features']\n",
    "    heads1=hyper_param_g['heads1']\n",
    "    heads2=hyper_param_g['heads2']\n",
    "    dropout_rateA=hyper_param_g['dropout_rateA']\n",
    "    dropout_rateB=hyper_param_g['dropout_rateB']\n",
    "    dropout_rateC=hyper_param_g['dropout_rateC']\n",
    "    dense_layer1=hyper_param_g['dense_layer1']\n",
    "    \n",
    "    model_a  = modelA(hidden_channels1,hidden_channels2, num_node_features,heads1,heads2\n",
    "                 ,dropout_rateA,dropout_rateB, dropout_rateC,dense_layer1)\n",
    "   \n",
    "    list_modelA.append(model_a)\n",
    "    \n",
    "# load model C (combined model)\n",
    "for i in range(k):\n",
    "    \n",
    "    model_a  = list_modelA[i]\n",
    "        \n",
    "    model_b  = list_modelB[i] # model b is pretrained model trainable = off \n",
    "    \n",
    "    total_input_features=hyper_param_c['total_input_features']\n",
    "    num_classes=hyper_param_c['num_classes']\n",
    "    dropout_rate_C=hyper_param_c['dropout_rate_C']\n",
    "    dense_layer_C=hyper_param_c[ 'dense_layer_C']\n",
    "    \n",
    "    combined_model = Combined_model(model_a, model_b, total_input_features, num_classes,dropout_rate_C, dense_layer_C) \n",
    "    \n",
    "    PATH = '0.75144model_hybrid'+ str(i)+'.pth'\n",
    "    combined_model.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "    list_modelC.append(combined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7bb4413d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\toxpred\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "The average of mcc : 0.61203\n",
      " \n",
      "The average of acc : 0.83226\n"
     ]
    }
   ],
   "source": [
    "nCV= 10 # ten crossfold validation \n",
    "list_fold_pred =[]\n",
    "list_fold_targets =[]\n",
    "list_feature_A =[]\n",
    "list_feature_C =[]\n",
    "\n",
    "v_test_loader = V_Loader(dataset = final_clean_fingerp_train[0], batch_size = 1)\n",
    "\n",
    "g_test_loader = G_Loader(dataset = data_list_train[0], batch_size = 1)\n",
    "\n",
    "\n",
    "for combined_model in list_modelC:  \n",
    "    list_pred, list_targets, list_A, list_C = test_1(g_test_loader, v_test_loader,combined_model)\n",
    "    list_fold_pred.append(list_pred)\n",
    "    list_fold_targets.append(list_targets)\n",
    "    list_feature_A.append(list_A)\n",
    "    list_feature_C.append(list_C)\n",
    "    \n",
    "    \n",
    "total_performances = performances(list_fold_pred, list_fold_targets, nCV)\n",
    "mcc = statistics.mean(total_performances[3])\n",
    "acc = statistics.mean(total_performances[0])\n",
    "print(\" \")\n",
    "print(f'The average of mcc : {mcc}')\n",
    "print(\" \")\n",
    "print(f'The average of acc : {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "546ff6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_feature_A[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "20f96729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_feature_C[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3eb5ca9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_train_targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cd837e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('dataX_A',list_feature_A[0])\n",
    "np.save('dataX_C',list_feature_C[0])\n",
    "np.save('dataY',total_train_targets[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1041397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
